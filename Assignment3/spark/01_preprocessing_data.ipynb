{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Preprocessing data\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- First: the original hashtags have been blanked out from the tweet. Build a predictive model that can predict the hashtag used based on the rest of the tweet text\n",
    "- Second: show that your model can make predictions in a deployed streaming setting using Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stream is text-based with each line containing one tweet (one instance) formatted as a JSON dictionary with the following keys (features): \"tweet_id\" (id of the tweet, not to be used), \"tweet_text\" (the tweet with the hashtags blanked out) and \"label\" (the target) You can use extra data and libraries if you want, but this is not required You are strongly encouraged to build your model using spark.ml (MLlib), but you can use scikit-learn as a fallback if things don't work out\n",
    "I.e. show that you can connect to the data source, preprocess/featurize incoming tweets, have your model predict the label, and show it, similar to \"spark_streaming_example_predicting.py.ipynb\" (but hopefully using a smarter, real predictive model) This means that you'll need to look for a way to save and load your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-PJSVL4LK:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#print(len(glob.glob(\"C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics/myoutput_draft/-*\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "\n",
    "sc.textFile along with spark.read.json collapse with the amount of tweets collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = sc.textFile(\"C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics/myoutput_draft/-*\",minPartitions=8 )\n",
    "#data.first() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.json(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to get all the files that were collected. Then, we filter only for the ones that have tweet info (files that do not include _SUCCESS or crc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = \"C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\"\n",
    "files=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(parent_path):\n",
    "    files.extend(os.path.join(dirpath, filename) for filename in filenames)\n",
    "\n",
    "files_filter=[]\n",
    "\n",
    "for ad in files: \n",
    "    if not \"_SUCCESS\" in ad and not \"crc\" in ad and not \".csv\" in ad:\n",
    "        files_filter.append(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.json(files_filter[0])\n",
    "\n",
    "i = 1\n",
    "while i < len(files_filter):\n",
    "  # create the csv writer object\n",
    "    for dirpath in files_filter[i:i+1]:\n",
    "        file=spark.read.json(dirpath)\n",
    "        try:\n",
    "            df3 = file.toPandas()\n",
    "        #    df1=df1.union(file)         \n",
    "        except TypeError:\n",
    "            pass   \n",
    "    df3.to_csv(\"C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics/tweet_05.csv\", mode= 'a', index= False, header = False)\n",
    "    i += 1\n",
    "    print(i, \"has been read\")\n",
    "else:\n",
    "    print(\"I'm done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
