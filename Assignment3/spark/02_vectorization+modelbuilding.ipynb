{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\marce\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9)\n",
      "Requirement already satisfied: jip in c:\\users\\marce\\anaconda3\\lib\\site-packages (0.9.15)\n",
      "Requirement already satisfied: requests in c:\\users\\marce\\anaconda3\\lib\\site-packages (from jip) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->jip) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->jip) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->jip) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->jip) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install jip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import pyspark\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext #To connect to spark\n",
    "from pyspark.sql import SparkSession #To connect to sql\n",
    "#from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-PJSVL4LK:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the path where all of our historical data is storaged. Here we are reading all files that have .csv extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_.csv',\n",
       " 'C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_00.csv',\n",
       " 'C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_01.csv',\n",
       " 'C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_02.csv',\n",
       " 'C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_03.csv',\n",
       " 'C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_04.csv',\n",
       " 'C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\\\\tweet_05.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_path = \"C:/Users/Marce/OneDrive - KU Leuven/Advanced Analytics\" \n",
    "all_tweets = glob.glob(parent_path + \"/*.csv\")\n",
    "all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all csv to create a single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10590</th>\n",
       "      <td>#china</td>\n",
       "      <td>1385644409971224579</td>\n",
       "      <td>What does #███████ want to do with #███████ ? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>#biden</td>\n",
       "      <td>1381634468788666368</td>\n",
       "      <td>#███████ said, \"I'm not content that a Chinese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>#china</td>\n",
       "      <td>1381659043341668359</td>\n",
       "      <td>The military threat from #███████ V  USA accor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>#vaccine</td>\n",
       "      <td>1381585331795808256</td>\n",
       "      <td>#███████ appts available at Rite Aid in: Kings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>#biden</td>\n",
       "      <td>1385613615655538688</td>\n",
       "      <td>Over $3.7 billion worth of crypto positions we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category             tweet_id  \\\n",
       "10590    #china  1385644409971224579   \n",
       "11917    #biden  1381634468788666368   \n",
       "288      #china  1381659043341668359   \n",
       "545    #vaccine  1381585331795808256   \n",
       "10204    #biden  1385613615655538688   \n",
       "\n",
       "                                              tweet_text  \n",
       "10590  What does #███████ want to do with #███████ ? ...  \n",
       "11917  #███████ said, \"I'm not content that a Chinese...  \n",
       "288    The military threat from #███████ V  USA accor...  \n",
       "545    #███████ appts available at Rite Aid in: Kings...  \n",
       "10204  Over $3.7 billion worth of crypto positions we...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.concat((pd.read_csv(file, header=None)\n",
    "          for file in all_tweets),ignore_index= True)\n",
    "df.columns = ['category','tweet_id', 'tweet_text']\n",
    "df = df.drop_duplicates(subset=['tweet_id'])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dimension of our dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11577, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the functionalities of Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- tweet_id: long (nullable = true)\n",
      " |-- tweet_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = sqlContext.createDataFrame(df)\n",
    "file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking the distribution of the hashtags collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      category|count|\n",
      "+--------------+-----+\n",
      "|        #biden| 1804|\n",
      "|      #vaccine| 3118|\n",
      "|        #china| 2411|\n",
      "|        #covid| 2887|\n",
      "|#stopasianhate|  967|\n",
      "|    #inflation|  390|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check several models: \n",
    "\n",
    "1. Logistic regression using TF-IDF features\n",
    "2. Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression using TF-IDF Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline:\n",
    "\n",
    "1. Tokenizer \n",
    "2. Stopwords\n",
    "3. HashingTF\n",
    "4. IDF\n",
    "5. Convert hashtag into numeric label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer1 = Tokenizer(inputCol=\"tweet_text\",\n",
    "                       outputCol=\"words\")  # separe words\n",
    "sw = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\") # removes stop words\n",
    "hashingTF = HashingTF(inputCol=\"filtered\",\n",
    "                      outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\",\n",
    "          minDocFreq=5) \n",
    "label_stringIdx = StringIndexer(\n",
    "    inputCol=\"category\", outputCol=\"label\")  # labels to numerics\n",
    "\n",
    "# set up pipeline\n",
    "pipeline = Pipeline(stages=[Tokenizer1, sw, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to split our dataset. We are utilizing 80% for training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = file.randomSplit([0.8, 0.2], seed = 1005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing on training data\n",
    "\n",
    "Passing our training data through the pipeline that we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(trainingData)\n",
    "dataset = pipelineFit.transform(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=20, \n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the pipeline used to utilize it further in the streaming step (if we choose this proc ofc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipelineFit.write().overwrite().save(os.path.join(parent_path, 'IDF_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testData.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our previous trained pipeline on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pipelineFit.transform(testData)\n",
    "\n",
    "predictions = lrModel.transform(dataset1)\n",
    "\n",
    "# just checking some predictions\n",
    "predictions.filter(predictions['prediction'] == 2) \\\n",
    "    .select(\"category\", \"filtered\", \"probability\", \"label\", \"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n=10, truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: Not really a good model 0.55 accuracy...so we are going to use a 5-fold cross validation with 5 to see if our accuracy increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=20, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=MulticlassClassificationEvaluator(), \\\n",
    "                    numFolds=5) \n",
    "\n",
    "# Run cross validations\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "dataset1 = pipelineFit.transform(trainingData)\n",
    "\n",
    "cvModel = cv.fit(dataset1)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "dataset_pred = pipelineFit.transform(testData)\n",
    "predictions = cvModel.transform(dataset_pred)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating the model in test set.\n",
    "evaluator.evaluate(predictions)\n",
    "acc_lr = evaluator.evaluate(predictions)\n",
    "predictions.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the logistic regression model for further implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.555961109001589"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.write().overwrite().save(os.path.join(parent_path, 'twt_pyspark_LRModel1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
